{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import zipfile, random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "filename = 'text8.zip'\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total 17,005,207 words for training consisted of array of string.\n",
    "- We need to convert this string array to digit array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 11668632], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430), ('two', 192644), ('is', 183153), ('as', 131815), ('eight', 125285), ('for', 118445), ('s', 116710), ('five', 115789), ('three', 114775), ('was', 112807), ('by', 111831)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 20\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We choose the most frequent `vocabulary_size` words. \n",
    "- `UNK` means the rest of them except the most frequenty words.\n",
    "- In this example, \n",
    "    1. UNK\n",
    "    1. the\n",
    "    1. of\n",
    "    1. and\n",
    "    1. one\n",
    "    1. in\n",
    "    1. a\n",
    "    1. to\n",
    "    1. zero\n",
    "    1. nine\n",
    "    1. two\n",
    "    1. is\n",
    "    1. as\n",
    "    1. eight\n",
    "    1. for\n",
    "    1. s\n",
    "    1. five\n",
    "    1. three\n",
    "    1. was\n",
    "    1. by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before data ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "After data [0, 0, 12, 6, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('Before data', words[:10])\n",
    "print('After data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above result shows converted result from array of string to array of digits.\n",
    "- Mapping between a word and a digit is the `dictionary`, that will appear below block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 3, 'five': 16, 'the': 1, 'zero': 8, 'nine': 9, 'by': 19, 'a': 6, 'to': 7, 'two': 10, 'three': 17, 'one': 4, 'UNK': 0, 'as': 12, 'of': 2, 'in': 5, 'was': 18, 'for': 14, 's': 15, 'eight': 13, 'is': 11}\n",
      "{0: 'UNK', 1: 'the', 2: 'of', 3: 'and', 4: 'one', 5: 'in', 6: 'a', 7: 'to', 8: 'zero', 9: 'nine', 10: 'two', 11: 'is', 12: 'as', 13: 'eight', 14: 'for', 15: 's', 16: 'five', 17: 'three', 18: 'was', 19: 'by'}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)\n",
    "print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the skip-gram model.\n",
    "\n",
    "This is used when we apply stochastic gradient method to skip gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['UNK', 'UNK', 'as', 'a', 'UNK', 'of', 'UNK', 'UNK']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['UNK', 'UNK', 'as', 'as', 'a', 'a', 'UNK', 'UNK']\n",
      "    labels: ['UNK', 'as', 'a', 'UNK', 'UNK', 'as', 'a', 'of']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
      "    labels: ['UNK', 'a', 'UNK', 'UNK', 'as', 'UNK', 'UNK', 'of']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
