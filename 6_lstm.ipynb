{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a9474bedc391>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.296176 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "xatusstaidpquwnlgnpdii hz ccz lyxffrrae h ij nzmca si frurwjc pzotfesmtpt  leob \n",
      "xy  tqaudjtlxvrioposf s  ydzi tolkmtefv umcayxstl pqgd bdcsvemnitaleh  fk  ivray\n",
      "j no s i i euntu lfilciofcenoixyj decoddcxie alaqxygmtplhoisijei i ubdmwnytdnc y\n",
      "yeniv lhijsmthijj euh grwt z tlmeex vv  hoiintexthusbpsy  mncatzubtc rp  oqeizrd\n",
      "bylerlttzb trc upcsr   uoe  n cmmk uaesmol f c fwaienrtysondyeewixrlrraacpulycs \n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.592966 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.88\n",
      "Validation set perplexity: 10.40\n",
      "Average loss at step 200: 2.245837 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 300: 2.090722 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 1.993334 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 500: 1.934567 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 1.911781 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 700: 1.860770 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 800: 1.821454 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 900: 1.833362 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1000: 1.830307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "icn to zero four hace ffordd for cist avoseai interitiie side bell and from now \n",
      "tion modeps colled seprective indorre he beluced of greaw the his finded opceses\n",
      "x that recomencile the beagrake shecarn ducwive eight forea he to the avericarow\n",
      "s comm truust yearousa and his shanfe rown seffered hounch and sovers ilabon con\n",
      "y fivu aududion most wrounder rrix they to mading ial the othe f cress sepf jwar\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100: 1.777764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1200: 1.757847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1300: 1.733726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1400: 1.746278 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1500: 1.737010 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.746348 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1700: 1.713329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1800: 1.677098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 1900: 1.646510 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2000: 1.698397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "is exsocampt moduc bist continged inity and  one five six mudich is innitional a\n",
      "alss re in lange the fiction first larner quensle v side also dayan as despemser\n",
      "s chaidely knowider that conorman stitl and wosse ryponical sucken a spaler un b\n",
      "ory co and provides function def sichively in almance htal dispanea fatt a gener\n",
      "stloting kndix welshargedst in the church foutond langu of mathir and mering and\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.686316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2200: 1.678127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.645854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2400: 1.660177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2500: 1.677024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2600: 1.653049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2700: 1.659126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2800: 1.651546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2900: 1.646594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3000: 1.650497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "le latlong exgamy mollowimall conute pala havis all bervang betonn ii was apoan \n",
      "corn he upales of with hardhiter suffve trmened force and or of coupted one nine\n",
      "kents the internal rysttoplos restayed pat sonsiwite acongytary be latur albivit\n",
      "king can continded convestion at a sapand carding trave and filts and cyriral bl\n",
      " govers conflaud the webe two commo all are phodularols direccenvill pace was tr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.629171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3200: 1.644073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.636023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3400: 1.664668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3500: 1.658382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.665061 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.646395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3800: 1.641819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.637888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4000: 1.651796 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "y the schores they is pron theoks elected cagracher dubor regeltratic has assidi\n",
      "jus prodects and givide with with he are the wassic in the pittitl other enals s\n",
      "herer usepen cerformentades bigrals the atser albebrations weadey the birties pa\n",
      " one eight five heada developsing in there shock many moving metwhorita provided\n",
      "perprefus brovanoce jumpa use all speck or becament naint of the clease marning \n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4100: 1.631132 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.638437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.614611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4400: 1.610412 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4500: 1.612794 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.609143 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4700: 1.624647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4800: 1.626174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4900: 1.632096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.605491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "parein incopenes he loughros one kings to a is from is mird misle exaded the man\n",
      "ly to the hwick or iul j dered mosturies to manyly and on alloxic probatter in o\n",
      "x latest whilewinver then yay form to the wint one nine four seen engent in unit\n",
      "real menth to otens earksontainert mast of b major the mides revelle was giving \n",
      "vers like cataurs with de and howay appean over of the mosts have it with state \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5100: 1.603593 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200: 1.589598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5300: 1.576666 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.579216 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5500: 1.563169 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5600: 1.577955 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5700: 1.569964 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.587020 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.574075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6000: 1.544882 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "ing and other benro and prob minsle tebring be cardored bay but some and itula b\n",
      "ond lives artisms sundestly marence the between obsentred detocall masuan and de\n",
      "im in the lip army b c in commond generally to a predict hugory on lifes one nin\n",
      "x use at for gals bitord is the norderer autious the ecrolle of in the upis of o\n",
      "m libritric application which of the works wy legall stmered aftardd broth such \n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100: 1.571226 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.539045 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.546509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6400: 1.541227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500: 1.561204 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600: 1.593732 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6700: 1.578178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800: 1.598850 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6900: 1.581378 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.580026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "gle the gragration in ideralish systems the see seolver in the congle word the p\n",
      "ader for om that new to the jearlated susconsioning to bottz the the lade of nec\n",
      "s funthna simmarus redances that because malle in caphey duris and bused for ame\n",
      "rage and sletsophes memotorign clusane itsomemitical armsivally lw bothranisua w\n",
      "ver and law rue justion of obtenment from a contelond formed in seven the bothch\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-43c8b13f6a78>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.299662 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.10\n",
      "================================================================================\n",
      "ore eiuugtwn niq njglrfg  hl enreyocixb h tyh bp cqi phruik fs eaee la j itxc di\n",
      "opvhk szjbutgramxmeeobcnpbjjdtz yk x e bbuhiura aesaxsoshrpiee i onve xgerudhyji\n",
      "fdhfag bymuor qhmcnhntybmsze  ajcl c wxzrasdqt exl zptolh ak rg rqnteah  vxov bi\n",
      "kz ta  jdog eaiznrbureqaix  pniaxei nswa eqdgfscw lksups iip ai  lgakhjk sfny sy\n",
      "moacdhmiukvlsjvp n ehnsvidp j si  gyiahjngdfjrwebot xctinx teksrakegarzxjd rq oe\n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 100: 2.574624 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.26\n",
      "Validation set perplexity: 10.14\n",
      "Average loss at step 200: 2.229203 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.21\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 300: 2.080873 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 2.032948 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.84\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 500: 1.980908 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 600: 1.898987 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.872773 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 800: 1.873768 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 900: 1.849508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1000: 1.848901 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "================================================================================\n",
      "k over use from nish in orans of orecies us bitions wind midcre one nine comre o\n",
      "endy worllriog musaq of his with to retary one rincypare force of gear hadpl bur\n",
      "ingh one three zero six use of chise s tl veus matelian wheger thele plemenor th\n",
      "mal lir soun gnogerumy distablies of one putions prelib th tor magy thisking wen\n",
      "t highansany hyquld stlie anabyers reparts for the consasces d puitory in in jan\n",
      "================================================================================\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1100: 1.804383 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1200: 1.775184 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1300: 1.763301 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1400: 1.761773 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1500: 1.749377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.730015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.718052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.691932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1900: 1.695869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2000: 1.678061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "l dovelands the treat as jeven india provedant da firet the most as the pertil m\n",
      "vers lans outs of the most be e eccerts of timation ts que sou conwain mottel he\n",
      "c with grant secures instract fon amon of do the dedry to deferencatic is illact\n",
      "z of eftend of the masse indo searchulad to boys six hir whenk in b gejs haman f\n",
      "ped bown confart ans i ows all from laid which most four siglic and for eipen ea\n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.688605 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2200: 1.705437 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2300: 1.708999 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2400: 1.690890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2500: 1.693369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2600: 1.674694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2700: 1.684633 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2800: 1.684582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2900: 1.676883 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3000: 1.686369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "lesso rysillem two zero zero mankia in evels a tear by inclased in mat tener out\n",
      "pare havt seven shiend steor that change as to test to have news the caprymers b\n",
      "man with see awinitical after of the kard of wide turn of as the exinistations a\n",
      "ubce is see specite of the use to amp by the gasa cast strage availate the aili \n",
      "no the memorlauk story be as duovivian whored which address froust it the ana po\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3100: 1.654786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3200: 1.637509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3300: 1.649529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3400: 1.634646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3500: 1.678409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3600: 1.652101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3700: 1.651923 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.658271 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.653920 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4000: 1.640073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "vers it dest society brink vedy for norma kayseshes when pagine open epietize in\n",
      "les being havew virting instance and offtc and post senerablaysism specif of the\n",
      "warce icrafuer substezolidences her amig incipaushity international ineelstion g\n",
      "ter numerory europeand of to knight two bet one zero two mammon often the butwin\n",
      " interndess indusoration was theature ows ideased of the list ecandse atans one \n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4100: 1.618638 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.618103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4300: 1.619066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4400: 1.614118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4500: 1.643407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4600: 1.626775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4700: 1.625468 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4800: 1.608837 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4900: 1.622238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5000: 1.616585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "quar romesly at hasso refencaning ligh acture polyrated their sps jata a hise si\n",
      "ched doop is be seit prispola to the other treps as all ve d other a deuses duli\n",
      "net the apgeed or of poorty dange cavorman that the sucstroy sa ly for of indyme\n",
      "great watels union by the either queens in elidersportaturey connementic italism\n",
      "ulises then spcotqhs ginficted dia for it rayed in vimiting to behating to unsta\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5100: 1.597184 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5200: 1.598057 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5300: 1.594725 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.589870 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5500: 1.590708 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5600: 1.561980 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5700: 1.578411 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5800: 1.595884 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5900: 1.584963 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6000: 1.586569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "ness norved marso methoved or existured commonly for is utture also relative wor\n",
      "lands of blighterspeares eight five him mack the stughtment over fads s to his i\n",
      "per new invert engloght arazarylig three systers during for it other orhagoonk a\n",
      "hotine ported sport theris it among of the loged by heather short s that the usi\n",
      "hich beasomeny progip the monules into the historifically goobs highararium on p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6100: 1.577900 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6200: 1.590788 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6300: 1.588344 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6400: 1.575550 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6500: 1.558117 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6600: 1.601542 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.574768 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6800: 1.575406 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6900: 1.574219 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.590758 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "y mpmine consiex laidshozo dop usilian a see madring buy into these eight by ref\n",
      "ing adds creat spysso thisfiel nalely the peneare raize such its mono by a ippod\n",
      "len of one five eighters however popidis humicary arla saypherfic currelity prol\n",
      "x obs gish as well vat theve the female of chiden by and protectro thus hlsyle s\n",
      "wozk depear in city accurvence infected  of the generan mych that numonival pres\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-43c8b13f6a78>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.312636 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.46\n",
      "================================================================================\n",
      "gycic s l yx a wj pteqgzwahki ozki qz i gbvawk qqns l s utusire z badeioc   ior \n",
      "e hsnnebnt e oxmrtt  thkqtile sv  cvraai dnemyy ss  sg stsh  obycacxmp i    vt l\n",
      "j  owt axto gqh klraj osx as afpzkas e e  x   zhq q q mwi plgbper syl afunctdd h\n",
      "vmascvcyhptuh d uitszomsl hvuet zt  loe mvu kbq  nmf j l nrhcwcrrarqnhbon it  da\n",
      "wgsrzayeqrjgneerx eh hdc p zxhdt ihev yctynmqjgpo haqbnhevs  aoooir j f dore ol \n",
      "================================================================================\n",
      "Validation set perplexity: 19.93\n",
      "Average loss at step 100: 2.296696 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.11\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 200: 2.020337 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 300: 1.918246 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 400: 1.865795 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 500: 1.885627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 600: 1.824551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 700: 1.804488 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 800: 1.793348 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 900: 1.789749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1000: 1.728763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "ple telles one nation enpard eopper or cartection in suchical sy japitial an one\n",
      "bod weasts one seven one the chyod centribide refunts on obterple the rigges alt\n",
      "que plest be outing only one nine one eunuring to or diis seqeambed usive plaing\n",
      "quers oried micial vael nemadmentide taking infutite one nine five six new the h\n",
      "ally trapes of the gay sen pree the presents head pades one nine five no zero on\n",
      "================================================================================\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1100: 1.705702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.740241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1300: 1.722136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1400: 1.696272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1500: 1.688281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1600: 1.688330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1700: 1.711236 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.682749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.682013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2000: 1.698066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "urring moth and poprove explutes could converneliam high inquned it hand mansant\n",
      "kne belis tracted becamehttionassly dooms andited chrownel bather twic cescurate\n",
      "namoraters d would been allowings on is were spect due blaw os land namesadess m\n",
      "y jugents hawes are hes bos knesignsument a laws mosence of consiber while fatua\n",
      "vicles to timeseds postence lestarnams corvicgles of the aya breats music two fo\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2100: 1.683680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2200: 1.659599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2300: 1.664153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2400: 1.676196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2500: 1.694467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2600: 1.667136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2700: 1.685848 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2800: 1.646999 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2900: 1.654704 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3000: 1.651862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "med arguinal he o julians was leadisted found sequrist alber used olted ebered t\n",
      "cjod exc lown island of goog bunic unarungly concection beeig ok code augusting \n",
      "quneing and because of ima alvimed werle sechemss his ulter westered its siusing\n",
      "logy islands is percial lindon of the unire lint emption of make three nime of e\n",
      "k fere comparetive and two zero x one thre six wered is bards lorian image m ber\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3100: 1.653561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3200: 1.653189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3300: 1.637173 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.640294 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3500: 1.634416 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3600: 1.631299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3700: 1.633526 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3800: 1.628740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3900: 1.627873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4000: 1.628989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "zmer abmo a languages this spocinss maling applutamente and km wayssageaine iii \n",
      "ficasts peasinles allowished understries spoposibely scimala with the respons sp\n",
      "xice power and ganually troduch chases class jeaching the nearly sume palitain i\n",
      "matic outputed in three seven nine comp horing the kpssuid two zero three zero k\n",
      "cernary all numberson s goll the populato upinstruce war tures bolls this lasper\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4100: 1.631198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4200: 1.624606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4300: 1.603701 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4400: 1.634190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4500: 1.641963 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4600: 1.647826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4700: 1.617727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4800: 1.600835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.615253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5000: 1.642368 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "x surfert troyle by themstasting a gas may bather the pasumation of the viblubia\n",
      "ward that retacily newart perfered to special large internative unitations vhaci\n",
      "ches dialdings tronsbleking held a all them of irea france definedia sabo classs\n",
      "xine with the he curchuard all molos a simple englistly grepsues compan as custe\n",
      "youped stimian because one shore zero two brin to the was pilop to a tauming and\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5100: 1.624961 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5200: 1.612080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5300: 1.571642 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5400: 1.572838 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5500: 1.566211 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5600: 1.588905 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5700: 1.544702 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5800: 1.553576 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5900: 1.569593 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.537369 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "zbrie the actions increek of houlder exedicat nadio lorose composed ammonting th\n",
      "ge use hist the fluteform in the munfilia akmology come shild jun that vies of p\n",
      "ray with a concent an other a such e at traces of the movies his a trang a stitu\n",
      "wates was fetrous of the slated a delticus a on spluding internal links on two f\n",
      "gen ropport more instead s expection green women to pusence f interpoit and forc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6100: 1.561270 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.582466 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6300: 1.592692 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6400: 1.622668 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6500: 1.618864 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6600: 1.588386 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6700: 1.573635 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6800: 1.556259 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6900: 1.547929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 7000: 1.563190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "e may one seven eight two one five zero an orther happs a yeages ruth type edua \n",
      "h german lant of fitor a but significal its rischiles of celly has pointer much \n",
      "drating and instruction be dapson see scongrass national stare spreceas in do th\n",
      "faiguers there crimical diagerson was betlectivities the a camethorian vesigness\n",
      "ricle one nine eight order scientificisted also the frexch murhagantiam pictured\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(0, train_labels).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-e1e6150f300c>:8 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.294304 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "ieavryenweteetwaw m wsuvgevh sucurtn e e e neiudbabpifsr o obbe  eehteqheyj aattt\n",
      "wawszb b cjadif stm yefdtl  zzcs  gq ljrrqmaa kzies fp vtent dixozlp lan kzarljte\n",
      "lleh qozjdmtuxdrofnntpr s apyd hyou ig seixfxoig   pmaoiz  plzuepfgbsof drntewix \n",
      "af dllfp creaidzhz ei  v pbhamwnzoha  iiccs oe lxact siwrsc aq uhpiag ogm  t netl\n",
      "ojr v ellhi yictowh a las wul y uauneotv m rddplran  qexcaqkic jxfi  dig aixevloe\n",
      "================================================================================\n",
      "Validation set perplexity: 19.93\n",
      "Average loss at step 100: 2.275372 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 200: 1.966673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 300: 1.884756 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 400: 1.826558 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 500: 1.761020 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 600: 1.762353 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 700: 1.741423 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 800: 1.726588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 900: 1.720467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 1000: 1.690012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      " forms  spectived descrime up of divollinical comiculum wested in norten esespeci\n",
      "ld yariste the given great proupered by the mintanzene in to benaand their estedi\n",
      "kted mes servavoyle agess as the sis egaher an efficome socree with dar out geopo\n",
      "sg the segle major nine contically of mardised to the shought commodels it the st\n",
      "hd from measing the onlie quench and offorged from naturaskeing communis withm an\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 1100: 1.695556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 1200: 1.689620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1300: 1.696447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 1400: 1.659384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 1500: 1.654440 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 1600: 1.641642 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1700: 1.649653 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 1800: 1.670035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1900: 1.651223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2000: 1.662400 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "================================================================================\n",
      "acky notates injdlent was gris funcabove led army time as autor clintil to eight \n",
      "vh wow which into after major rdne nine three eight seven play only thitory and d\n",
      "jhar the more new also succeed the lania amples were and swit state that two visc\n",
      "nmentfortant nongelt consiralia culturely liter soon cretech as  and red not char\n",
      " land there paster watints relature of postled aview eigurus of the two than anth\n",
      "================================================================================\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2100: 1.650598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2200: 1.665879 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 2300: 1.650644 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 2400: 1.639520 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 2500: 1.649730 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 2600: 1.639808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 2700: 1.622263 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2800: 1.628304 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2900: 1.620269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 3000: 1.642085 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "gface reteral which countonia years recond gods he swing fross muches and john de\n",
      "pen them in a see and prographic the koplays not and his and popular becames form\n",
      "ifican major producersythdresel joinering the seasoon collegegeles with politicit\n",
      "v the sore and even frava have conscity in ronsterent and expromony insitions of \n",
      "xsories effencence confirst frongs while of cachrental cities orlars s mostries c\n",
      "================================================================================\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 3100: 1.609996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3200: 1.625826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3300: 1.621901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 3400: 1.615092 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3500: 1.605659 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 3600: 1.624480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3700: 1.595715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3800: 1.599580 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 3900: 1.586962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 4000: 1.604941 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "cor persat a sponsisy its internation signade that contindition of the one nineut\n",
      "craphy seldon process that them robof ded fouls octicloses tor officians to and r\n",
      "twor quanton casing bott niey and eagley inst gords saa seven herpy by fore fathe\n",
      "ters and consferring pology settle weljtoth is seands has a was one four one nine\n",
      "kpe austriousparns one nine five thought because was metched an althilon in eagge\n",
      "================================================================================\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 4100: 1.615044 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4200: 1.596824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 4300: 1.567064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 4400: 1.593482 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4500: 1.581682 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 4600: 1.580472 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 4700: 1.598711 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4800: 1.592588 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 4900: 1.616614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5000: 1.617218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "rds comperlinguoeturned from interring movies the secord imse smit peoptation tra\n",
      "oment live offirms is dural rarely west the religions common ii earlieu morisions\n",
      "spaled are iwrombers of fester one nine five fictions and of the pa set was not j\n",
      "mvsm usets a do which by the lyre free movain picenthions the treeps budges and o\n",
      "even few making medica two beal total a game boys il more others of three mastoya\n",
      "================================================================================\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 5100: 1.581265 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5200: 1.590243 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5300: 1.564977 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 5400: 1.559933 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5500: 1.557715 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5600: 1.542071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5700: 1.572936 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5800: 1.562075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5900: 1.571967 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6000: 1.532629 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "================================================================================\n",
      "ed after from their directure aphore rom gimed yug nomical stillow yaust fast the\n",
      "xbilitary finer dresider the south are some devil ux five four s a time of bosion\n",
      "natip fro the locations then the bluse of their councish hand this his one eight \n",
      "kly number of can in the extintle much ago fred a brown of the linux livers about\n",
      "hqee aircraftcale game the has brownighted also see loracker commit form six six \n",
      "================================================================================\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6100: 1.583702 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6200: 1.579508 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 6300: 1.564524 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 6400: 1.579351 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6500: 1.577218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 6600: 1.566597 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6700: 1.558411 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6800: 1.569556 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6900: 1.602856 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 7000: 1.587469 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "gbn flood holdly nadound we s docuthematicimay estbasily take san bii brannygiste\n",
      "ighetan instant the invl source in ordinal had a purpoes to b sharvates out his r\n",
      "fv bown ham this and muslip is such how number of this to forwaving via which had\n",
      "zuptor perom one five isothers horward long its was tegmron for shout wpets it se\n",
      "yme and mean size linu kond phroser image of the war that is than the tumation an\n",
      "================================================================================\n",
      "Validation set perplexity: 6.61\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-f615653ac2f6>:8 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.307894 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.33\n",
      "================================================================================\n",
      "jo ug fjls trhoknsnrnj mvzxev  tbsjchaeahehzix  xh f vhlolk mroganhte xayhi   hnc\n",
      "mnhkme emaceouqumoegn eeapdine xqf eefeiidqwfku rg   zjvjgraqujlb le ews w pr  ya\n",
      "wk pbc cpczpsdxipwntst g k fj rej a ofolivpe s iek xcewgbnm ftdeslnguenstv  uel f\n",
      "jn iayvo in lrenar de rqweb crnezo  mjcdpajrt u drewoetjwhzwie y tgujjss   evoap \n",
      "etn ynu  nktfwzcrdwy eicse ssrnb biwnhs k vbtimkoembhgnyi ohi vzoz skamaoemutcagq\n",
      "================================================================================\n",
      "Validation set perplexity: 19.88\n",
      "Average loss at step 100: 2.292267 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.99\n",
      "Validation set perplexity: 9.33\n",
      "Average loss at step 200: 1.970255 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 300: 1.875590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 1.827302 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 500: 1.796458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 600: 1.753610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 700: 1.744381 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 800: 1.707664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 900: 1.706694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1000: 1.695590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "knowas arty one ninnuning bield to for two recote mained provive and the cas heuo\n",
      "nus genoth pitian pravide subseartics bamer dolking an certs wrishis paine and ev\n",
      "hvaries of new ne severent protokls of germed by three two seven two zero nine ha\n",
      "fht onseven hower monothymen this and yine mostey statian by mainder effectincien\n",
      "qssue dists is stitunization basuch occan owproces of comprolic and matheir acter\n",
      "================================================================================\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 1100: 1.690986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 1200: 1.676702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 1300: 1.665161 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 1400: 1.664919 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 1500: 1.689465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 1600: 1.677792 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 1700: 1.652827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 1800: 1.687779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 1900: 1.677076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 2000: 1.643887 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "================================================================================\n",
      "wge of the is well and its arcolgeral botbetelled on ot hall the family the a rol\n",
      " mestly joing the austrination if per to from of charactore and eaculm six two fo\n",
      "yman includents secutors amurhere with proportant cranks of one seven nive zero t\n",
      "lhlowsmalizes a praced stance to dembrandistries ion a prinpsoul amounginese in i\n",
      "apined at also they world current lowecep and achosperia to daugwust by germans h\n",
      "================================================================================\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 2100: 1.635228 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 2200: 1.623433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 2300: 1.661555 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 2400: 1.650893 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 2500: 1.626413 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 2600: 1.615867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 2700: 1.616933 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 2800: 1.626958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 2900: 1.606424 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 3000: 1.601317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      " powest paint europeen this increaser game two currenticle this inflanguncibial t\n",
      "lwith more death one nine seve river used the lack campi empire clayntpt brall or\n",
      "pxistially jacentoy as two four two french railin the first when facibillintrains\n",
      "lhefenic renary where telendition cleve often around native for their langus on t\n",
      "qkic canorely kiunts year of editalle two book large lived a new condustrility of\n",
      "================================================================================\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 3100: 1.632910 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 3200: 1.628580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 3300: 1.616134 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 3400: 1.611465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 3500: 1.603067 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3600: 1.579585 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 3700: 1.592965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 3800: 1.604225 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 3900: 1.620695 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 4000: 1.605845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "wf to be prevents refence is disevered abuoain barly still knowledge edward this \n",
      "qfnhad increasts micros a to onsix feaths as are reke have population he worke ha\n",
      "knowed the sone of his s sential deported primaly this dievlies and man regions a\n",
      "uke drities bovonstantify smerical in convonderns of horsaissevility sime thather\n",
      "uucan and lesrusal shernations be concil of the india a systeonem subsrom hinderm\n",
      "================================================================================\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 4100: 1.609661 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 4200: 1.588332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 4300: 1.595742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4400: 1.603689 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 4500: 1.602234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 4600: 1.590738 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4700: 1.592394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 4800: 1.606589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 4900: 1.593630 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 5000: 1.607153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "nry french the we bodwery of allended in them and profilme turt his part telester\n",
      "zvolly of fs of two ecesseven most micts guess the some more jania makaum the all\n",
      "as has fried of the law cort uport been oppoln performan processes i addringer bl\n",
      " cph challese some of conternational shundal if uniquirtinten wom one nine zero i\n",
      "gp with the people its guensisiond just hets appearapta an ancets starse army the\n",
      "================================================================================\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 5100: 1.599956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 5200: 1.598713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 5300: 1.592272 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 5400: 1.571342 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 5500: 1.583915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 5600: 1.605717 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 5700: 1.582641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 5800: 1.575922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 5900: 1.588081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 6000: 1.597059 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "vo the but clubased mexicol ination inkaefye aemperoling guild orboorience of the\n",
      "pdh as chiccer disticalsogy rans as beleven ben an eiggotabledy the needs told wi\n",
      "res audigent henraming compantor film foundation and sexperision moned staguide p\n",
      "oes given of to the squagestifically ba great of figlargesl also five with eight \n",
      "xkt his iv used sinterperive of are rose   immentanther that grown in ordergionbu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 6100: 1.608493 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 6200: 1.589389 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 6300: 1.598753 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 6400: 1.631943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 6500: 1.637418 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 6600: 1.606124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 6700: 1.609130 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 6800: 1.592082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 6900: 1.558758 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 7000: 1.597537 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "fc slic ore to john most and out sachigotons in the chron a b one fries one nine \n",
      "ge peterington a congress to the living b has the effect country unites and hetwo\n",
      "jtnes mez infectius only mightnubt one of to bess for five seven nine four june e\n",
      "ul be regularly authoricational functed be as applium prider in expern b one four\n",
      "tly for the cenjastaur ece setties central fries they pended of the rea of are an\n",
      "================================================================================\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 7100: 1.589241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 7200: 1.584555 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 7300: 1.602300 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 7400: 1.593504 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 7500: 1.589653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 7600: 1.581723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 7700: 1.588272 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 7800: 1.601377 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 7900: 1.611552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 8000: 1.605410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "qctic become lenting river twenzern a is on the britism paulhose reduce in was us\n",
      "awtion the empero an evolve ang garasirs to suk our and boards a pearl bahrate to\n",
      "moral alther unition commernfnoth rook hard a swith manders his one kauthas one n\n",
      "ecw or techno alla baseing a rexamples which as clay disive history not when occu\n",
      "ocally courbal the contecture the robaaellow of juditional in who into be on clau\n",
      "================================================================================\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 8100: 1.580535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 8200: 1.582710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 8300: 1.600444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 8400: 1.594207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 8500: 1.606538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 8600: 1.611042 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 8700: 1.595744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 8800: 1.609959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 8900: 1.589020 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 9000: 1.591553 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "cnsie etok and  o ufdled peflective caption compating has into the god united the\n",
      "jc cause nine evephy in a current lift six one zero five zero more zer japan film\n",
      "xrese recented in which of eurgetree regute tale ephined onser experiment it drac\n",
      "fvalleans cound to during locodi teasure actually occupoting warms cmicros shortu\n",
      "ying namenel it english to and relikes that actior cultic he periphqmylearner fou\n",
      "================================================================================\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 9100: 1.598695 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 9200: 1.619694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 9300: 1.607821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 9400: 1.598207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 9500: 1.606567 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 9600: 1.605955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 9700: 1.612943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 9800: 1.608943 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 9900: 1.572480 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 10000: 1.591385 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "sympid famous the and the as in base called detain bonwal lack in there b two one\n",
      "scaln in the leechyal level water slinoiding without the his racificants in the u\n",
      "sgt enronocers can tweut walways expi infragey as sicles composes that the causin\n",
      "m to two one severo kiled appearly all two zero zero home the author publishing d\n",
      "qk pans from three sampirem pathroman ovice populations within seven influences a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 10100: 1.610173 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 10200: 1.602724 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 10300: 1.591483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 10400: 1.603532 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 10500: 1.614140 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10600: 1.560408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 10700: 1.576217 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 10800: 1.590160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 10900: 1.602624 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 11000: 1.575642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "wn similarion and bygters you of audyeatimal country three five descessers broadd\n",
      "wousalemy thing resorterian angeles in onicell only into contrommunication of dis\n",
      "pjethis chang agentuenced skilor defined ent of events his categort soviet accent\n",
      "ediscloseling aishine doublic were requensity a not cents produced and the falled\n",
      "olvcs islaces of australicially acenced thodoutcycistory ports emologarm remulati\n",
      "================================================================================\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 11100: 1.561242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 11200: 1.562551 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 11300: 1.552936 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 11400: 1.562744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 11500: 1.568503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 11600: 1.537113 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 11700: 1.543497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 11800: 1.564994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 11900: 1.554841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 12000: 1.539481 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "================================================================================\n",
      "rs reed by hafters two is roots all the pe the see the performed in advent stativ\n",
      "me and use system attemss eefer six untillies the miglear nearrian guilt zeroom t\n",
      "kto call as the wenreform of the the wilved normites anks with subse because of t\n",
      "hat could them the he value the wyator me one caldwell metates that ideef itselfu\n",
      "mberties attack on mathence one three three mys general examplir have in this her\n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 12100: 1.539836 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 12200: 1.561008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 12300: 1.555343 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12400: 1.593482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 12500: 1.560970 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 12600: 1.554782 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 12700: 1.553199 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 12800: 1.566400 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 12900: 1.590616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 13000: 1.564756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "ykllow planmenth is the communiqueometer an emipsitiah zero zero zero x islan beg\n",
      "vfts mounts est of participate annohnce cented eroumy roental sociah future one w\n",
      "ying to chronication mhry polly or lights distural jonadiocurrent precondentary h\n",
      "kyrgyzstan in denting finally ssembly circular regtment is notable is he over and\n",
      "lms play also storpolitical recognity main involved st truhonazia anude there one\n",
      "================================================================================\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 13100: 1.559986 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 13200: 1.602026 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 13300: 1.584581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 13400: 1.587476 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 13500: 1.600766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 13600: 1.588435 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 13700: 1.558349 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 13800: 1.535117 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 13900: 1.566354 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 14000: 1.563379 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "================================================================================\n",
      "gqe was howevert god eight seven five five five additional carry aptions degarch \n",
      "uded by ovirvers and members tradity and identita the partation axr united the re\n",
      "scenes logy party war wort one trialy when keen to they the found the houousument\n",
      "bit a legislementationalty her some collarcent death hood bomprowering vot family\n",
      "yqabilysice or the new required mays avero no thelectly is however of the present\n",
      "================================================================================\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 14100: 1.577643 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 14200: 1.583001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 14300: 1.574755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 14400: 1.585405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 14500: 1.613913 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 14600: 1.590030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 14700: 1.608189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 14800: 1.586942 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 14900: 1.589472 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 15000: 1.579050 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "================================================================================\n",
      " present varyagrone nitch carring furts name the the minite upters according one \n",
      "lzsorts directudiety consevere homs d and set brew transpirs the howed one ner ze\n",
      "nried today the not faspacker dry boot tophore one seven in which it the u s bet \n",
      "wder r s was dirsting wint of chlor the nationals monorai of the musive a model b\n",
      "eature crapher hero the pilm all inspirigy chaling crimes storation of black aid \n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15100: 1.548118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 15200: 1.565809 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 15300: 1.538845 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 15400: 1.545232 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 15500: 1.507592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 15600: 1.520628 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 15700: 1.516671 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 15800: 1.502678 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 15900: 1.520264 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 16000: 1.532696 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "jqed gunnibaleadom ca a cwsy revoily general apostles is depeferd which the root \n",
      "xro standards like capage honigaths documentadify becoming nated from the plays g\n",
      "rn presenturating judga from to der to the most the first s dwin mount of known e\n",
      "ka and than merge write contain politicy export for creat and marren a subconcert\n",
      "ies known to internafe hearted dangeld to overiton two and three of the f indisef\n",
      "================================================================================\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 16100: 1.523737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 16200: 1.496184 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 16300: 1.477348 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 16400: 1.518895 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 16500: 1.527927 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 16600: 1.520773 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 16700: 1.561374 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 16800: 1.509252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 16900: 1.523946 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 17000: 1.531830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "adings theore and arour one convident psychationality s wien or disagree one five\n",
      "was amatery in tradamma imporard strial lositude a vary of the his included the p\n",
      "aulqei or k madmed governments association i one seven six eight three one nine j\n",
      "hr energinifies call spawared long hoandrd lore mous nomil who promise ds first h\n",
      "kgregood to the literainer differentiation which lor keeps come orgented thinted \n",
      "================================================================================\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 17100: 1.519311 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 17200: 1.541418 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 17300: 1.551095 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 17400: 1.589111 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 17500: 1.567757 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 17600: 1.587253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 17700: 1.579694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 17800: 1.563624 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 17900: 1.551103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 18000: 1.522319 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "================================================================================\n",
      "cj kinid remains and fraximal alleguessed golders of further hom flord in one nin\n",
      "ome goard most receigned this good of an othely have that the can primar is uncha\n",
      "ijaking  episcovergy fadesiss miched to the relation a large by the competition s\n",
      " x and gone them s physice took was romwere reel livy showese light and fug has a\n",
      "pkark of the united to estables the tagely not brited one only this hell statue c\n",
      "================================================================================\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 18100: 1.518257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 18200: 1.540569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 18300: 1.543014 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 18400: 1.569002 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 18500: 1.564344 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 18600: 1.574460 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 18700: 1.566405 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 18800: 1.569151 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 18900: 1.551895 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 19000: 1.598931 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "mzaels arch of a president from they crices write one two one innor problemnes of\n",
      "zsual limerical paround conclusion of rands of the christian his actnomans a sapl\n",
      "binities the demociaj relations and partings title not systemuch in logicizing af\n",
      "pywritoral united annii compose charbiage of right of artand government followed \n",
      "ygher to information aqeedes together named to haa a s extraimed to that french e\n",
      "================================================================================\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 19100: 1.583147 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 19200: 1.556436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 19300: 1.554072 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 19400: 1.533995 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 19500: 1.533531 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 19600: 1.548563 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 19700: 1.560031 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 19800: 1.537079 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 19900: 1.548409 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 20000: 1.518543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "sabot and were listern dict are enships in pers to histans is here meution are ch\n",
      "tds had hellod and characulcomadia one nine eight eight one when the ne blickage \n",
      "rhe govenistree but gpzyes pursu aptive avoine one nine the a splace worpoba ship\n",
      "nne s in the like maintal role wide of the mearn more to be early the indiature o\n",
      "iyard two two zero zero zero they effects engester instage biobal structric gener\n",
      "================================================================================\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 20100: 1.527335 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 20200: 1.526268 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 20300: 1.552295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 20400: 1.552353 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 20500: 1.544529 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 20600: 1.509785 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 20700: 1.501622 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 20800: 1.518257 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 20900: 1.519106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 21000: 1.517326 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "fk and cards signation of d slap code theugest in aration on the learned theater \n",
      "gp of include cross related to congres smaa the intest three four eight face on a\n",
      "ywas not it in kistancal publishe numpt social varumal usevation seven country ar\n",
      "aq inteltisplel scer with however thwere the society and lanning i was wherely re\n",
      "ej s between bookptimed thee companiyeuke almost incrafe in the british countier \n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
